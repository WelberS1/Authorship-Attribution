{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdef6798-5e08-4863-b7ab-61ca731271f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "from time import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, ConcatDataset, DataLoader\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51b6eae1-677b-41bd-bafe-837528b7639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_time = time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c69661d-1946-45f4-bb9c-7bf1f6882356",
   "metadata": {},
   "source": [
    "# Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "363ff03e-5035-4198-887d-d1d95375df35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_path):\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "\n",
    "train_file_path = \"Data/train.json\"\n",
    "train_dataset = load_file(train_file_path)\n",
    "\n",
    "test_file_path = \"Data/test.json\"\n",
    "test_dataset = load_file(test_file_path)\n",
    "\n",
    "train_dataset_list = []\n",
    "for value in train_dataset.values():\n",
    "    train_dataset_list.append(value)\n",
    "\n",
    "test_dataset_list = []\n",
    "for value in test_dataset.values():\n",
    "    test_dataset_list.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "843067f8-7bdc-4dbf-8304-48f4e4a85b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "venue_number = 471\n",
    "keywords_number = 500\n",
    "\n",
    "author_max = -1\n",
    "author_min = 100000\n",
    "year_earlier = 3000\n",
    "year_latest = 0\n",
    "for value in train_dataset.values():\n",
    "    for author in value[\"author\"]:\n",
    "        if author_max < author:\n",
    "            author_max = author\n",
    "        if author_min > author:\n",
    "            author_min = author\n",
    "\n",
    "    year = value[\"year\"]\n",
    "    if year_earlier > year:\n",
    "        year_earlier = year\n",
    "    if year_latest < year:\n",
    "        year_latest = year\n",
    "\n",
    "year_number = year_latest - year_earlier + 1\n",
    "author_number = author_max - author_min + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f087d41-ff25-496c-8e75-ab8f748c9773",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b95598-ee87-4a4a-b25d-2b671faef7a1",
   "metadata": {},
   "source": [
    "## Onehot + TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50772c78-4311-4418-9d2c-cd946efa8dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataset_list):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for instance in dataset_list:\n",
    "        authors = instance[\"author\"]\n",
    "        for targe_author in authors:\n",
    "            dic = copy.deepcopy(instance)\n",
    "            aut = copy.deepcopy(authors)\n",
    "            aut.remove(targe_author)\n",
    "            dic[\"author\"] = aut\n",
    "            X.append(dic)\n",
    "            Y.append(targe_author)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "822b4913-a3fa-4013-b0e1-489fdffd34ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainData(Dataset):\n",
    "    def __init__(self, dataset_list):\n",
    "        self.X, self.Y = get_dataset(dataset_list)\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.Y[index]\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "\n",
    "class MyTestData(Dataset):\n",
    "    def __init__(self, dataset_list):\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        for data in dataset_list:\n",
    "            self.X.append({\n",
    "                \"venue\": data[\"venue\"],\n",
    "                \"keywords\": data[\"keywords\"],\n",
    "                \"year\": data[\"year\"],\n",
    "                \"author\": data[\"coauthor\"]\n",
    "            })\n",
    "            self.Y.append(data[\"target\"])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.Y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04f5f3bf-e88c-47bc-90c6-7d081cd7c123",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_mydata = MyTrainData(train_dataset_list)\n",
    "test_dataset_mydata = MyTestData(test_dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d553adc-1414-41f8-a5eb-10f56783a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = open(\"Data/TFIDF_ALL.json\", \"r\")\n",
    "word_bag_all = json.load(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8595ca2-e26a-459b-815a-bcaf84542a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_function(dataset_tuple):\n",
    "    venues = []\n",
    "    keywords = []\n",
    "    authors = []\n",
    "    Y = []\n",
    "\n",
    "    for x, y in dataset_tuple:\n",
    "        \n",
    "        venue_one_hot = [0 for i in range(venue_number)]\n",
    "        venue = x[\"venue\"]\n",
    "        if venue != \"\":\n",
    "            venue_one_hot[venue] = 1\n",
    "        else:\n",
    "            venue_one_hot[venue_number - 1] = 1\n",
    "        venues.append(venue_one_hot)\n",
    "\n",
    "        # keywords\n",
    "        keywords_data = x[\"keywords\"]\n",
    "        keywords_tfidf = [0 for i in range(keywords_number)]\n",
    "        for k in keywords_data:\n",
    "            word_bag = word_bag_all\n",
    "            if word_bag.get(str(k)) is None:\n",
    "                keywords_tfidf[k] = 0\n",
    "            else:\n",
    "                keywords_tfidf[k] = word_bag.get(str(k)) * 50\n",
    "        keywords.append(keywords_tfidf)\n",
    "\n",
    "\n",
    "        # author\n",
    "        author_one_hot = [0 for i in range(author_number)]\n",
    "        if x[\"author\"]:\n",
    "            for author in x[\"author\"]:\n",
    "                author_one_hot[author] = 1\n",
    "        authors.append(author_one_hot)\n",
    "        \n",
    "        Y.append(y)\n",
    "\n",
    "    X_output = []\n",
    "    for i in range(len(venues)):\n",
    "        X_output.append(venues[i] + keywords[i] + authors[i])\n",
    "    \n",
    "    result = {\n",
    "        \"X\": torch.FloatTensor(X_output),\n",
    "        \"labels\": torch.LongTensor(Y)\n",
    "    }\n",
    "    \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc93a652-23cc-4f24-adc6-7c60ccbe7749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecb5bf42-4598-4e4a-ab06-effa26066b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset_mydata, batch_size=16, shuffle=True, collate_fn=collate_function)\n",
    "test_dataloader = DataLoader(test_dataset_mydata, batch_size=16, collate_fn=collate_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb2c435-72a4-4466-b221-3101eea3c4c6",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f34a26f-92aa-40a4-afa2-f97a75b6ae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.module = nn.Sequential(\n",
    "            nn.Linear(input_dim, keywords_number+author_number),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(keywords_number+author_number, keywords_number+author_number - 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(keywords_number+author_number - 256, author_number),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data_batch):\n",
    "        \n",
    "        result = self.module(data_batch[\"X\"])\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc2a08a-480f-425a-8081-6ca617dc7468",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea15c0dc-fccb-4e35-bc7c-33ee60e5b98a",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17759d05-8bc7-4ed7-9960-3a6404523778",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MyClassifier(input_dim=venue_number+keywords_number+author_number)\n",
    "if torch.cuda.is_available():\n",
    "    clf = clf.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cdf4e7-6008-4cad-bab4-56ec90095671",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "950f5ff1-6176-4826-8424-be9cd8c42908",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "if torch.cuda.is_available():\n",
    "    loss_function = loss_function.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad235ef-9e8c-4fc5-991f-4cc1f133f1b0",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6583c0a-5c56-4a8a-b3e9-b1b4c67ef366",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(clf.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dbda9c-e49c-4d8f-991b-8fa807d259bb",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1903dcd6-2a96-44e1-b2da-2caac4ac3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 20\n",
    "\n",
    "for i in range(epoch):\n",
    "    print(\"------ Epoch {} start ------\".format(i + 1))\n",
    "    for data in train_dataloader:\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_batch = {'X': data['X'].cuda()}\n",
    "            gpu_batch['labels'] = data['labels'].cuda()\n",
    "            data = gpu_batch\n",
    "        outputs = clf(data)\n",
    "        loss = loss_function(outputs, data[\"labels\"])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "            \n",
    "torch.save(clf.state_dict(), f\"NN.mdl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cde494-99ca-4d33-a71c-234a659b7d7c",
   "metadata": {},
   "source": [
    "## Predistions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67aa798-9804-48a1-a084-5dfd2a6ccd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = []\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        log_pre = clf(data)\n",
    "        pre = torch.exp(log_pre)\n",
    "        for i in range(len(pre)):\n",
    "            predicts.append(pre[i][data['labels'][i]].detach().squeeze().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c903f-71a5-4629-9362-a6442f8a93b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time()\n",
    "run_time = end_time-begin_time\n",
    "print(run_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aa20aa-8aea-4c5c-8466-5c165c33f79c",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4893380-a415-40e5-b65d-5b34e8d540dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Predictions/predict-NN-multi-10.csv','w',encoding='utf-8')\n",
    "csv_writer = csv.writer(f)\n",
    "csv_writer.writerow([\"ID\",\"Predicted\"])\n",
    "for i in range(len(predicts)):\n",
    "    csv_writer.writerow([i, predicts[i]])\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f61be11-fec4-454f-8652-e983b2baefa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}